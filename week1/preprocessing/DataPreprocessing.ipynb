{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ad9a2e",
   "metadata": {},
   "source": [
    "## 1. Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a59c698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tensorflow_datasets in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (4.9.7)\n",
      "Requirement already satisfied: absl-py in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: click in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Requirement already satisfied: dm-tree in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (0.1.9)\n",
      "Requirement already satisfied: immutabledict in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (4.2.1)\n",
      "Requirement already satisfied: numpy in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (1.26.4)\n",
      "Requirement already satisfied: promise in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (4.25.3)\n",
      "Requirement already satisfied: psutil in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: pyarrow in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (19.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (2.32.3)\n",
      "Requirement already satisfied: simple-parsing in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (1.16.1)\n",
      "Requirement already satisfied: termcolor in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: toml in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (4.67.1)\n",
      "Requirement already satisfied: wrapt in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (1.17.0)\n",
      "Requirement already satisfied: array-record>=0.5.0 in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow_datasets) (0.6.0)\n",
      "Requirement already satisfied: etils>=1.9.1 in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.12.0)\n",
      "Requirement already satisfied: fsspec in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.10.0)\n",
      "Requirement already satisfied: importlib_resources in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.5.2)\n",
      "Requirement already satisfied: typing_extensions in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (4.12.2)\n",
      "Requirement already satisfied: zipp in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow_datasets) (2024.8.30)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from dm-tree->tensorflow_datasets) (25.1.0)\n",
      "Requirement already satisfied: six in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from simple-parsing->tensorflow_datasets) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.66.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e00c9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "# import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from keras_preprocessing.text import text_to_word_sequence\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60512fd",
   "metadata": {},
   "source": [
    "## 2. Import Preprocessor module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c28800c",
   "metadata": {},
   "source": [
    "### Basic cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d118463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and tokenize\n",
    "def clean_text(text, tokenizer, stopwords):\n",
    "    \"\"\"Pre-process text and generate tokens\n",
    "    \n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        Tokenized text.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()                            # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)               # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)                    # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)                  # Remove ellipsis (and last word)\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)         # Replace dash between words\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  # Remove punctuation\n",
    "\n",
    "    tokens = tokenizer(text)                                            # Get tokens from text\n",
    "    tokens = [t for t in tokens if not t in stopwords]                  # Remove stopwords\n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]                 # Remove digits\n",
    "    tokens = [t for t in tokens if len(t) > 1]                          # Remove short tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f750e68",
   "metadata": {},
   "source": [
    "### Writing to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19a09d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_write(X , y , dataname ):\n",
    "    train_data = pd.DataFrame(list(zip(X, y)))\n",
    "    train_data.to_csv(f'../data/{dataname}.csv')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df630b8",
   "metadata": {},
   "source": [
    "## 3. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a5cf26",
   "metadata": {},
   "source": [
    "**IMDb Reviews** is a large dataset for binary sentiment classification, consisting of 50,000 highly polar reviews (in English) with an even number of examples for training and testing purposes.\n",
    "\n",
    "The dataset contains additional unlabelled data. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. No more than 30 reviews are included per movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5142e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/anaconda3/envs/hieupcvp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e836d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tfds.load('imdb_reviews', as_supervised=False)\n",
    "dataset = load_dataset(\"aayya/sst2-augmented\")\n",
    "ds = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cf3910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dataset = ds['train_orig']\n",
    "orig_eda_dataset = ds['train_orig_eda']\n",
    "orig_eda_embedding_dataset = ds['train_orig_eda_embedding']\n",
    "orig_eda_embedding_wordnet_dataset = ds['train_orig_eda_embedding_wordnet']\n",
    "validate_datset = ds['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be39351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_orig', 'train_orig_eda', 'train_orig_eda_embedding', 'train_orig_eda_embedding_wordnet', 'val'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8bcdac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dat = pd.DataFrame(dataset['train'])\n",
    "# test_dat = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# X_train, y_train = pd.DataFrame([s for s in train_dat['sentence']], columns=['text']), train_dat['label']\n",
    "# X_test, y_test = pd.DataFrame([s for s in test_dat['sentence']], columns=['text']), test_dat['label']\n",
    "\n",
    "def get_Xy(datadict , dataname):\n",
    "    dataset = datadict[f'{dataname}']\n",
    "    X , y = pd.DataFrame([s for s in dataset['sentence']], columns = ['text']) , dataset['label'] \n",
    "    return X , y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa5604d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(X):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    X['text'] = X['text'].map(lambda x:clean_text(x, word_tokenize, stop_words))\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63179f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# X_train['text'] = X_train['text'].map(lambda x:clean_text(x, word_tokenize, stop_words))\n",
    "# X_test['text'] = X_test['text'].map(lambda x:clean_text(x, word_tokenize, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe753e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[rock, destined, 21st, century, new, conan, go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[gorgeously, elaborate, continuation, lord, ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[singercomposer, bryan, adams, contributes, sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[think, america, would, enough, plucky, britis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[yet, act, still, charming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8539</th>\n",
       "      <td>[real, snooze]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8540</th>\n",
       "      <td>[surprises]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8541</th>\n",
       "      <td>[seen, hippie, turned, yuppie, plot, enthusias...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8542</th>\n",
       "      <td>[fans, walked, muttering, words, like, horribl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8543</th>\n",
       "      <td>[case, zero]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8544 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     [rock, destined, 21st, century, new, conan, go...\n",
       "1     [gorgeously, elaborate, continuation, lord, ri...\n",
       "2     [singercomposer, bryan, adams, contributes, sl...\n",
       "3     [think, america, would, enough, plucky, britis...\n",
       "4                           [yet, act, still, charming]\n",
       "...                                                 ...\n",
       "8539                                     [real, snooze]\n",
       "8540                                        [surprises]\n",
       "8541  [seen, hippie, turned, yuppie, plot, enthusias...\n",
       "8542  [fans, walked, muttering, words, like, horribl...\n",
       "8543                                       [case, zero]\n",
       "\n",
       "[8544 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e3ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.69444\n",
       "1       0.83333\n",
       "2       0.62500\n",
       "3       0.50000\n",
       "4       0.72222\n",
       "         ...   \n",
       "8539    0.11111\n",
       "8540    0.22222\n",
       "8541    0.75000\n",
       "8542    0.13889\n",
       "8543    0.34722\n",
       "Name: label, Length: 8544, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b73269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_write(X , y , dataname , output_dir='/mnt/disk1/hieupcvp/RNN/SentimenalAnalysis/data'):\n",
    "    # Ensure X_train and X_test are DataFrames with a 'text' column\n",
    "    if isinstance(X, pd.DataFrame) and 'text' in X.columns:\n",
    "        X_train_text = X['text'].apply(lambda tokens: ' '.join(tokens))  # Convert tokens to strings\n",
    "    else:\n",
    "        raise ValueError(\"X_train must be a DataFrame with a 'text' column.\")\n",
    "    \n",
    "    # if isinstance(X_test, pd.DataFrame) and 'text' in X_test.columns:\n",
    "    #     X_test_text = X_test['text'].apply(lambda tokens: ' '.join(tokens))  # Convert tokens to strings\n",
    "    # else:\n",
    "    #     raise ValueError(\"X_test must be a DataFrame with a 'text' column.\")\n",
    "    \n",
    "    # Create DataFrames for training and testing data\n",
    "    train_data = pd.DataFrame({'text': X_train_text, 'label': y})\n",
    "    # test_data = pd.DataFrame({'text': X_test_text, 'label': y_test})\n",
    "    \n",
    "    # Write to CSV files in the new output directory\n",
    "    train_data.to_csv(f'{output_dir}/{dataname}.csv', index=False)\n",
    "    # test_data.to_csv(f'{output_dir}/TestSet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e857caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # exporting resultant datasets\n",
    "# file_write(X_train, X_test, y_train, y_test)\n",
    "\n",
    "for dataname in ds.keys():\n",
    "    X , y = get_Xy(ds , dataname)\n",
    "    X = remove_stopwords(X)\n",
    "    file_write(X, y , dataname)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hieupcvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
